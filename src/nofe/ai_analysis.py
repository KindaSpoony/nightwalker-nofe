# src/nofe/ai_analysis.py
import os
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

DEFAULT_MODEL = "gpt-5"
FALLBACK_MODELS: Tuple[str, ...] = ("gpt-5-preview", "gpt-4o", "gpt-4o-mini")

SYSTEM_PROMPT = (
    "You are a senior OSINT analyst. Given a CHAOS report generated by NOFE, "
    "provide an executive analysis: identify the most important narratives, "
    "highlight geopolitical or economic risks, cross-reference entities, flag potential "
    "misinformation, and suggest follow-up questions. Be concise but insightful."
)

def load_report(report_path: str) -> str:
    return Path(report_path).read_text(encoding="utf-8")

def _get_api_key(cfg: Optional[Dict] = None) -> Optional[str]:
    return os.getenv("OPENAI_API_KEY") or (cfg or {}).get("openai_api_key")

def _chat_new_client(api_key: str, model: str, messages, temperature: float, max_tokens: int) -> str:
    # New OpenAI SDK (>=1.0): from openai import OpenAI
    from openai import OpenAI
    client = OpenAI(api_key=api_key)
    resp = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return resp.choices[0].message.content.strip()

def _chat_legacy(api_key: str, model: str, messages, temperature: float, max_tokens: int) -> str:
    # Legacy SDK (<1.0): import openai
    import openai
    openai.api_key = api_key
    resp = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return resp["choices"][0]["message"]["content"].strip()

def _normalize_model_name(model: Optional[str]) -> str:
    if not model:
        return DEFAULT_MODEL
    normalized = model.strip()
    lower = normalized.lower()
    if lower == "gpt-5":
        return "gpt-5"
    if lower == "gpt-5-preview":
        return "gpt-5-preview"
    if lower == "gpt-4o":
        return "gpt-4o"
    if lower == "gpt-4o-mini":
        return "gpt-4o-mini"
    return normalized


def _iter_models(preferred: str) -> Iterable[str]:
    seen: List[str] = []

    def _push(candidate: str):
        if candidate and candidate not in seen:
            seen.append(candidate)

    _push(preferred)
    for fallback in FALLBACK_MODELS:
        _push(fallback)
    return seen


def _should_retry_with_fallback(exc: Exception) -> bool:
    message = str(exc).lower()
    if "model" not in message:
        return False

    try:
        import openai  # type: ignore

        model_errors = (
            getattr(openai, "BadRequestError", Exception),
            getattr(openai, "NotFoundError", Exception),
            getattr(openai, "PermissionDeniedError", Exception),
        )
        if isinstance(exc, model_errors):
            keywords = (
                "does not exist",
                "not exist",
                "not found",
                "no access",
                "access to the model",
                "is not available",
                "unknown model",
            )
            return any(kw in message for kw in keywords)
    except Exception:
        # If we cannot introspect error types, fall back to keyword detection only.
        pass

    keywords = ("not exist", "not found", "no access", "is not available", "unknown model")
    return any(kw in message for kw in keywords)


def _call_chat_completion(api_key: str, model: str, messages, temperature: float, max_tokens: int) -> str:
    last_error: Optional[Exception] = None
    for fn in (_chat_new_client, _chat_legacy):
        try:
            return fn(api_key, model, messages, temperature=temperature, max_tokens=max_tokens)
        except Exception as exc:  # pragma: no cover - exercised in CI failures
            last_error = exc
    assert last_error is not None  # for type checkers
    raise last_error


def _format_failure_message(failures: List[Tuple[str, Exception]]) -> str:
    lines = ["AI analysis skipped: unable to use configured OpenAI models."]
    for model_name, exc in failures:
        lines.append(f"- {model_name}: {exc}")
    return "\n".join(lines)


def generate_ai_analysis(report_text: str, cfg: Optional[Dict] = None) -> str:
    """
    Return AI analysis string. If no API key is available, return a skip message.
    """
    api_key = _get_api_key(cfg)
    if not api_key:
        return "AI analysis skipped: missing OPENAI_API_KEY."

    preferred_model = _normalize_model_name((cfg or {}).get("ai_model", DEFAULT_MODEL))
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": report_text[:120000]},  # safety truncation
    ]

    failures: List[Tuple[str, Exception]] = []
    for model in _iter_models(preferred_model):
        try:
            return _call_chat_completion(api_key, model, messages, temperature=0.4, max_tokens=1000)
        except Exception as exc:
            failures.append((model, exc))
            if not _should_retry_with_fallback(exc):
                break

    return _format_failure_message(failures)

def main(report_path: str, output_path: str, cfg: Optional[Dict] = None):
    text = load_report(report_path)
    analysis = generate_ai_analysis(text, cfg=cfg)
    Path(output_path).write_text("# AI Analysis\n\n" + analysis + "\n", encoding="utf-8")

if __name__ == "__main__":
    import argparse, yaml
    parser = argparse.ArgumentParser()
    parser.add_argument("report_path")
    parser.add_argument("--output_path", default="")
    args = parser.parse_args()
    # Optional: pull cfg so ai_model / openai_api_key can be used locally
    base = Path(__file__).resolve().parent
    cfg_path = base / "config.yaml"
    cfg = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) if cfg_path.exists() else {}
    out = args.output_path or args.report_path.replace("CHAOS_", "AI_CHAOS_")
    main(args.report_path, out, cfg=cfg)
